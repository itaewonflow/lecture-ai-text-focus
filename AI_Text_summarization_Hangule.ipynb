{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itaewonflow/lecture-ai-text-focus/blob/main/AI_Text_summarization_Hangule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcb13b8a"
      },
      "source": [
        "- github : https://github.com/itaewonflow\n",
        "- Explain : https://decadepivot.com/\n",
        "---"
      ],
      "id": "dcb13b8a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 트랜스포머 모형을 이용한 [한글] 문서 요약"
      ],
      "metadata": {
        "id": "yZ-qQfRSsorE"
      },
      "id": "yZ-qQfRSsorE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ00WMhi1p0z"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ],
      "id": "aJ00WMhi1p0z"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "LWwxw33Bmsms"
      },
      "id": "LWwxw33Bmsms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ee27db"
      },
      "source": [
        "## 한글 문서 요약"
      ],
      "id": "b4ee27db"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln9dPj9ZZQVG"
      },
      "source": [
        "- 한글 문서를 요약하는 것은 기본적으로 영어 문서와 동일하다. 가장 중요한 차이는 한글 말뭉치에 대해 학습된 토크나이저와 사전학습 언어모델이 있어야 한다는 것이다."
      ],
      "id": "ln9dPj9ZZQVG"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5fdab7be"
      },
      "outputs": [],
      "source": [
        "text = \"인공지능 또는 AI는 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현하려는 컴퓨터 과학의 세부분야 중 하나이다. 정보공학 분야에 있어 하나의 인프라 기술이기도 하다.인간을 포함한 동물이 갖고 있는 지능 즉, natural intelligence와는 다른 개념이다. 인간의 지능을 모방한 기능을 갖춘 컴퓨터 시스템이며, 인간의 지능을 기계 등에 인공적으로 시연(구현)한 것이다. 일반적으로 범용 컴퓨터에 적용한다고 가정한다. 이 용어는 또한 그와 같은 지능을 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 과학 기술 분야를 지칭하기도 한다.\"\n",
        "preprocess_text = text.strip().replace(\"\\n\",\"\")"
      ],
      "id": "5fdab7be"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KoBART\n",
        "- 사전학습 언어모델: gogamaza/kobart-summarization\n",
        "- 토크나이저와 모형: PreTrained ToKenizerFast와 BartForConditionalGeneration사용"
      ],
      "metadata": {
        "id": "gao17y161IeD"
      },
      "id": "gao17y161IeD"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddceacd8",
        "outputId": "b7643898-e557-47f9-d27b-ba254a8b7a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인간의 지능 지능을 모방한 기능을 갖춘 컴퓨터 시스템이며, 인간의 지능을 기계 등에 인공적으로 시연(구현)한 것이다. 일반적으로 범용 컴퓨터에 적용한다고 가정한다.\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
        "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
        "\n",
        "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\")\n",
        "summary_ids = model.generate(tokenized_text,\n",
        "                             num_beams=4, # beam의 길이\n",
        "                             no_repeat_ngram_size=3, #동어 반복을 피하기 위해 사용\n",
        "                             min_length=10,  #요약문의 최소 토큰 수\n",
        "                             max_length=150,  #요약문의 최대 토큰 수\n",
        "                             early_stopping=True) #EOS 토큰을 만나면 종료\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(summary)"
      ],
      "id": "ddceacd8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###mT5\n",
        "- 사전학습 언어모델: csebuetnlp/mT5_multilingual_XLSum\n",
        "- 토크나이저와 모형: AutoTokenizer와 AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "y-CXY8zS188_"
      },
      "id": "y-CXY8zS188_"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee2e0fe3",
        "outputId": "f9cd59f7-5c35-44fe-cb72-a36fa7583f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인공지능(AI)는 인간의 지능을 모방한 컴퓨터 시스템이다.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
        "\n",
        "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\")\n",
        "summary_ids = model.generate(tokenized_text,\n",
        "                             num_beams=4, # beam의 길이\n",
        "                             no_repeat_ngram_size=2, #동어 반복을 피하기 위해 사용\n",
        "                             min_length=10,  #요약문의 최소 토큰 수\n",
        "                             max_length=150,  #요약문의 최대 토큰 수\n",
        "                             early_stopping=True) #EOS 토큰을 만나면 종료\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(summary)"
      ],
      "id": "ee2e0fe3"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "de5c66b24b88d6fd3b2df5c0958b0d2eda73064658479df36c55d94ea0f77ab4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}